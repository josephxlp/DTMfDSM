{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate ROI tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, basename\n",
    "from os import makedirs,cpu_count\n",
    "from glob import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import fiona\n",
    "from rasterio.features import shapes\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import numpy as np\n",
    "\n",
    "def gdal_polygonize(tif_path, gpkg_path):\n",
    "    cmd = f'gdal_polygonize.py {tif_path} -b 1 -f \"GPKG\" {gpkg_path}'# OUTPUT DN'\n",
    "    os.system(cmd)\n",
    "\n",
    "def dissolve_features(input_gpkg,output_gpkg):\n",
    "    #input_layer, \n",
    "    \"\"\"\n",
    "    Dissolve all features in a GeoPackage layer into a single geometry.\n",
    "\n",
    "    :param input_gpkg: Path to the input GeoPackage file.\n",
    "    :param input_layer: Name of the layer to dissolve.\n",
    "    :param output_gpkg: Path to the output GeoPackage file with dissolved features.\n",
    "    \"\"\"\n",
    "    # Load the layer from the GeoPackage\n",
    "    gdf = gpd.read_file(input_gpkg)#, layer=input_layer)\n",
    "    \n",
    "    # Dissolve all features into a single geometry\n",
    "    dissolved = gdf.dissolve()\n",
    "    \n",
    "    # Save the dissolved geometry to a new GeoPackage\n",
    "    dissolved.to_file(output_gpkg, driver='GPKG') #layer='dissolved',\n",
    "    print(f\"Dissolved features saved to {output_gpkg}\")\n",
    "\n",
    "def buffer_features(input_gpkg,output_gpkg, buffer_distance):\n",
    "    \"\"\"\n",
    "    Apply a buffer to the dissolved features in a GeoPackage layer.\n",
    "\n",
    "    :param input_gpkg: Path to the input GeoPackage file with dissolved features.\n",
    "    :param input_layer: Name of the layer to buffer.\n",
    "    :param output_gpkg: Path to the output GeoPackage file with buffered features.\n",
    "    :param buffer_distance: Distance for the buffer operation.\n",
    "    \"\"\"\n",
    "    # Load the dissolved layer from the GeoPackage\n",
    "    gdf = gpd.read_file(input_gpkg)#, layer=input_layer)\n",
    "    \n",
    "    # Apply the buffer operation\n",
    "    buffered = gdf.buffer(buffer_distance)\n",
    "    \n",
    "    # Create a new GeoDataFrame to store the buffered geometry\n",
    "    buffered_gdf = gpd.GeoDataFrame(geometry=buffered, crs=gdf.crs)\n",
    "    \n",
    "    # Save the buffered geometry to a new GeoPackage\n",
    "    buffered_gdf.to_file(output_gpkg,driver='GPKG')# layer='buffered', \n",
    "    print(f\"Buffered features saved to {output_gpkg}\")\n",
    "\n",
    "def save_to_gpkg(gdf, output_path):\n",
    "    gdf.to_file(output_path, driver='GPKG')\n",
    "\n",
    "def raster2vectorcutline(raster_path, gpkg_path):\n",
    "    # Convert raster to polygons\n",
    "\n",
    "    gpkg_path_int = gpkg_path.replace('.gpkg', '_P.gpkg')\n",
    "    fb = gpkg_path_int.replace('.gpkg','_B.gpkg')\n",
    "    gdal_polygonize(raster_path, gpkg_path_int)\n",
    "    buffer_features(gpkg_path_int, fb, 0)\n",
    "    dissolve_features(fb,gpkg_path)\n",
    "    os.remove(gpkg_path_int)\n",
    "    os.remove(fb)\n",
    " \n",
    "    print(\"Process completed. Output saved to:\", gpkg_path)\n",
    "\n",
    "def clip_cultilinex(vpath,rpath, fopath):\n",
    "    cmd = f'gdalwarp -cutline {vpath} -crop_to_cutline {rpath} {fopath}'\n",
    "    #-dstalpha \n",
    "    os.system(cmd)\n",
    "\n",
    "def clip_cultiline(vpath, rpath, fopath):\n",
    "  # Open the vector file\n",
    "  with fiona.open(vpath, \"r\") as shapefile:\n",
    "      # Extract the geometry from the vector file\n",
    "      shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "  # Open the raster file\n",
    "  with rasterio.open(rpath) as src:\n",
    "      # Clip the raster with the geometry\n",
    "      out_image, out_transform = mask(src, shapes, crop=True)\n",
    "      out_meta = src.meta.copy()\n",
    "\n",
    "      # Update the metadata to reflect the new dimensions\n",
    "      out_meta.update({\n",
    "          \"driver\": \"GTiff\",\n",
    "          \"height\": out_image.shape[1],\n",
    "          \"width\": out_image.shape[2],\n",
    "          \"transform\": out_transform\n",
    "      })\n",
    "\n",
    "      # Write the clipped raster to a new file\n",
    "      with rasterio.open(fopath, \"w\", **out_meta) as dest:\n",
    "          dest.write(out_image)\n",
    "\n",
    "def get_out_params(tiles_dpath,bfile,src_dataset):\n",
    "    dname = basename(bfile).replace('.gpkg','').split('_')[-1]\n",
    "    dst_dpath = join(tiles_dpath,dname)\n",
    "    makedirs(dst_dpath, exist_ok=True)\n",
    "    dst_dataset = join(dst_dpath,basename(src_dataset))\n",
    "    return dst_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/LiDAR/Tonlesap_TSAP1.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/LiDAR/Tonlesap_TSAP2.tif']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdir = \"/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/\"\n",
    "lidar_dpath = join(wdir, 'LiDAR')\n",
    "tiles_dpath = join(wdir, 'TILES')\n",
    "\n",
    "ds_dpath = \"/media/ljp238/12TBWolf/ARCHIEVE/OUT/TILES12B/N13E103/\"\n",
    "vrt_files = ds_files = glob(f'{ds_dpath}/*.tif'); print(len(ds_files))\n",
    "\n",
    "cpus = int(cpu_count() - 3)\n",
    "basefiles = lidar_files = glob(f'{lidar_dpath}/*.tif')\n",
    "lidar_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bands(rpath):\n",
    "    with rasterio.open(rpath) as src:\n",
    "        b = src.count\n",
    "    print(f'{b}::{rpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tfile in ds_files:check_bands(tfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate gpkg from tifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_list = []\n",
    "for j,fi in enumerate(basefiles):\n",
    "    fo = fi.replace('.tif', '.gpkg')\n",
    "    print(fi,'\\n',fo)\n",
    "    gpkg_list.append(fo)\n",
    "   # raster2vectorcutline(fi,fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clip raster to cutiline gpkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_files = glob(f'{lidar_dpath}/*.gpkg')\n",
    "gpkg_files,lidar_dpath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ti = time.perf_counter()\n",
    "    with ProcessPoolExecutor(cpus) as PPE:\n",
    "        for i,vrt in enumerate(vrt_files):\n",
    "            src_dataset = vrt\n",
    "            #print(vrt)\n",
    "            for j,bfile in enumerate(gpkg_files):\n",
    "                dst_dataset = get_out_params(tiles_dpath,bfile,src_dataset)\n",
    "                #clip_cultiline(src_dataset,bfile,dst_dataset)\n",
    "\n",
    "                PPE.submit(\n",
    "                   clip_cultilinex,bfile,src_dataset,dst_dataset\n",
    "                )\n",
    " \n",
    "               \n",
    "    tf = time.perf_counter() - ti\n",
    "    print(f'run.time : {tf/60}  min(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "def get_tiles(ds, tile_width, tile_height, overlap):\n",
    "  nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "  xstep = tile_width - overlap\n",
    "  ystep = tile_height - overlap\n",
    "  for x in range(0, nols, xstep):\n",
    "      if x + tile_width > nols:\n",
    "          x = nols - tile_width\n",
    "      for y in range(0, nrows, ystep):\n",
    "          if y + tile_height > nrows:\n",
    "              y = nrows - tile_height\n",
    "          window = windows.Window(x, y, tile_width, tile_height)\n",
    "          transform = windows.transform(window, ds.transform)\n",
    "          yield window, transform\n",
    "\n",
    "def process_tile(args):\n",
    "  window, transform, src_meta, in_path, out_path, output_filename = args\n",
    "  metadata = src_meta.copy()\n",
    "  metadata['transform'] = transform\n",
    "  metadata['width'], metadata['height'] = window.width, window.height\n",
    "  out_filepath = os.path.join(out_path, output_filename.format(window.col_off, window.row_off))\n",
    "  \n",
    "  with rio.open(in_path) as src:\n",
    "      with rio.open(out_filepath, 'w', **metadata) as dst:\n",
    "          dst.write(src.read(window=window))\n",
    "\n",
    "def calculate_offset(width, height, tile_width, tile_height, stride_x, stride_y):\n",
    "  X = [x for x in range(0, width, stride_x)]\n",
    "  Y = [y for y in range(0, height, stride_y)]\n",
    "  offsets = list(itertools.product(X, Y))\n",
    "  return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TSAP1', 'TSAP2']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psize = 64\n",
    "WDIR = \"/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/\"\n",
    "tile_width = psize\n",
    "tile_height = psize\n",
    "overlap = 0\n",
    "output_filename = 'tile_{}_{}.tif'\n",
    "tilenames = os.listdir(WDIR)\n",
    "tilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tilenames(WDIR, gpkg_files):\n",
    "    tilenames = []\n",
    "    for i in gpkg_files:\n",
    "        bname = os.path.basename(i).split('/')[-1]\n",
    "        bname = bname.split('_')[-1].replace('.gpkg', '')\n",
    "        #print(bname)\n",
    "        o = os.path.join(WDIR, bname)\n",
    "        os.makedirs(o,exist_ok=True)\n",
    "        tilenames.append(o)\n",
    "    return tilenames\n",
    "#tilenames = gen_tilenames(WDIR, gpkg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stile = \"N13E103\"\n",
    "for tilename in tilenames:\n",
    "    tif_dpath = os.path.join(WDIR, tilename)\n",
    "    patch_dpath = os.path.join(tif_dpath, f'P{psize}')\n",
    "    os.makedirs(patch_dpath, exist_ok=True)\n",
    "    tif_files = glob(f'{tif_dpath}/*.tif')\n",
    "    for in_path in tif_files:\n",
    "        out_path = os.path.join(patch_dpath, os.path.splitext(os.path.basename(in_path))[0])\n",
    "        out_path = out_path.replace(f'{Stile}_','')\n",
    "        print(out_path)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "        with rio.open(in_path) as src:\n",
    "            src_meta = src.meta.copy()\n",
    "            width, height = src_meta['width'], src_meta['height']\n",
    "            stride_x = tile_width - overlap\n",
    "            stride_y = tile_height - overlap\n",
    "            offsets = calculate_offset(width, height, tile_width, tile_height, stride_x, stride_y)\n",
    "            tiles = [(windows.Window(x, y, tile_width, tile_height), windows.transform(windows.Window(x, y, tile_width, tile_height), src.transform)) for x, y in offsets]\n",
    "\n",
    "        args = [(window, transform, src_meta, in_path, out_path, output_filename) for window, transform in tiles]\n",
    "\n",
    "        with Pool(processes=cpu_count()) as pool:\n",
    "            pool.map(process_tile, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate df paths with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def create_tif_dataframe(base_path):\n",
    "  # Dictionary to hold lists of full paths to TIFF files for each subdirectory\n",
    "  data = {}\n",
    "\n",
    "  # Walk through the directory\n",
    "  for root, dirs, files in os.walk(base_path):\n",
    "      # Get the relative path of the current directory\n",
    "      relative_path = os.path.relpath(root, base_path)\n",
    "\n",
    "      # Initialize a list for the current directory if not already present\n",
    "      if relative_path not in data:\n",
    "          data[relative_path] = []\n",
    "\n",
    "      # Filter and add full paths of TIFF files to the list\n",
    "      for file in files:\n",
    "          if file.endswith('.tif') or file.endswith('.tiff'):\n",
    "              full_path = os.path.join(root, file)\n",
    "              data[relative_path].append(full_path)\n",
    "\n",
    "  # Create a DataFrame from the dictionary\n",
    "  df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def calculate_nan_ratio(file_path):\n",
    "  if pd.isna(file_path):\n",
    "      return np.nan\n",
    "\n",
    "  with rasterio.open(file_path) as src:\n",
    "      data = src.read(1)\n",
    "      # Set nodata values to NaN\n",
    "      nodata_value = src.nodata\n",
    "      if nodata_value is not None:\n",
    "          data[data == nodata_value] = np.nan\n",
    "\n",
    "      # Set values < -30 and > 700 to NaN\n",
    "      data[data < -30] = np.nan\n",
    "      data[data > 700] = np.nan\n",
    "\n",
    "      # Calculate the ratio of NaN values to total pixels\n",
    "      nan_ratio = np.isnan(data).sum() / data.size\n",
    "\n",
    "  return nan_ratio\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def calculate_nan_ratios_parallel(file_paths):\n",
    "  with ThreadPoolExecutor() as executor:\n",
    "      nan_ratios = list(executor.map(calculate_nan_ratio, file_paths))\n",
    "  return nan_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARNAMES = ['cdem_DEM', 'cdem_WBM', 'edem_LCM', 'edem_W84', 'EGM08', 'EGM96',\n",
    "       'label_thresh_ldem', 'label_thresh_pdem', 'multi_ALDTM', 'multi_ESAWC',\n",
    "       'NegroAOIDTM', 'SENTINEL1', 'tdem_COM', 'tdem_DEM', 'tdem_DEM_M',\n",
    "       'tdem_DEM_M_BIN', 'tdem_HEM', 'tdem_WAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['TSAP1', 'TSAP2'], '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rio_names = os.listdir(WDIR)\n",
    "rio_names,WDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpaths_list = []\n",
    "for i in range(len(rio_names)):\n",
    "    base_path = os.path.join(WDIR, rio_names[i],'P64')\n",
    "    dfpaths = create_tif_dataframe(base_path)\n",
    "    dfpaths = dfpaths[VARNAMES]\n",
    "    fpath = base_path.replace('P64', 'P64_dfpaths.csv')\n",
    "\n",
    "    a_ratios = calculate_nan_ratios_parallel(dfpaths['multi_ALDTM'])\n",
    "    dfpaths['multi_ALDTM_ratio'] = a_ratios\n",
    "\n",
    "    b_ratios = calculate_nan_ratios_parallel(dfpaths['tdem_DEM_M'])\n",
    "    dfpaths['tdem_DEM_M_ratio'] = b_ratios\n",
    "    print(fpath)\n",
    "    dfpaths.to_csv(fpath, index=False)\n",
    "    dfpaths_list.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = pd.concat(map(pd.read_csv, dfpaths_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_ALDTM_ratio\n",
       "0.000000    23\n",
       "0.015625     5\n",
       "1.000000     3\n",
       "0.006348     1\n",
       "0.029541     1\n",
       "0.839844     1\n",
       "0.128906     1\n",
       "0.468750     1\n",
       "0.056396     1\n",
       "0.002686     1\n",
       "0.175293     1\n",
       "0.179199     1\n",
       "0.004150     1\n",
       "0.001465     1\n",
       "0.012207     1\n",
       "0.018555     1\n",
       "0.020508     1\n",
       "0.034912     1\n",
       "0.210693     1\n",
       "0.047363     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpaths.multi_ALDTM_ratio.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_ALDTM_ratio\n",
       "0.000000    269\n",
       "1.000000     69\n",
       "0.015625      7\n",
       "0.187500      3\n",
       "0.921875      2\n",
       "           ... \n",
       "0.027832      1\n",
       "0.909180      1\n",
       "0.046631      1\n",
       "0.058594      1\n",
       "0.047363      1\n",
       "Name: count, Length: 95, dtype: int64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpath.multi_ALDTM_ratio.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Workflow TAB \n",
    "- BinClass\n",
    "- RegDI\n",
    "- RegID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOLX = ['EGM08','EGM96','tdem_HEM','edem_W84'] # add later 'SENTINEL1',\n",
    "FCOLY = ['label_thresh_ldem','multi_ALDTM']#'ZDIF']\n",
    "FCOLA = FCOLX + FCOLY\n",
    "DFPATHS = dfpath[FCOLA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pyspatialml import Raster\n",
    "\n",
    "def get_patch(df, idx):\n",
    "    return df.loc[idx:idx,:]\n",
    "\n",
    "def get_patch_paths(df, idx,cols=None):\n",
    "    if cols is None:\n",
    "        return df.loc[idx:idx,:].values[0].tolist()\n",
    "    else:\n",
    "        df = df[cols].copy()\n",
    "        return df.loc[idx:idx,:].values[0].tolist()\n",
    "\n",
    "\n",
    "def tif2df(patch_paths,cols):\n",
    "    patch_paths = [i for i in patch_paths for j in cols if j in i]\n",
    "    assert len(patch_paths) == len(cols), \"Length unequal\"\n",
    "    ds = Raster(patch_paths)\n",
    "    ds.names = cols\n",
    "   # tb = ds.to_pandas()\n",
    "    return  ds.to_pandas()\n",
    "\n",
    "def data_split(df_paths, test_pct=0.05, valid_pct=0.10):\n",
    "    train_valid_paths, test_paths = train_test_split(df_paths, test_size=test_pct, random_state=130222)\n",
    "    # Split train+valid into train (85%) and valid (10%)\n",
    "    train_paths, valid_paths = train_test_split(train_valid_paths, test_size=valid_pct, random_state=130222)\n",
    "    # Reset indices\n",
    "    train_paths = train_paths.reset_index(drop=True)\n",
    "    valid_paths = valid_paths.reset_index(drop=True)\n",
    "    test_paths = test_paths.reset_index(drop=True)\n",
    "\n",
    "    return train_paths, valid_paths, test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 6) (5, 6) (3, 6)\n"
     ]
    }
   ],
   "source": [
    "train_paths, valid_paths, test_paths =  data_split(DFPATHS.sample(frac=.1))\n",
    "print(train_paths.shape, valid_paths.shape, test_paths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/EGM08/tile_1216_384.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/EGM96/tile_1472_128.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/tdem_HEM/tile_192_640.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/edem_W84/tile_1472_192.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/label_thresh_ldem/tile_1216_384.tif',\n",
       " '/media/ljp238/12TBWolf/ARCHIEVE/OUT/TONLESAP/TILES/TSAP1/P64/multi_ALDTM/tile_128_320.tif']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_paths = get_patch_paths(valid_paths,1,FCOLA)\n",
    "patch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the two band problem ::: they should have just one \n",
    "# check the tiles where came from and more \n",
    "# all the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Raster(patch_paths)\n",
    "len(list(ds.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "di = tif2df(patch_paths,FCOLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_thresh_ldem\n",
       "1.0    4094\n",
       "0.0       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di['label_thresh_ldem'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgeoml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
